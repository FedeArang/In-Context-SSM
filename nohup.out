wandb: Currently logged in as: khaefeli. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240523_133007-91eatmj0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/khaefeli/incontextssm
wandb: üöÄ View run at https://wandb.ai/khaefeli/incontextssm/runs/91eatmj0
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240523_184418-bj23rskc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/khaefeli/incontextssm
wandb: üöÄ View run at https://wandb.ai/khaefeli/incontextssm/runs/bj23rskc
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

Exception in thread Thread-10 (_thread_body):
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_checksum.py", line 84, in _thread_body
    shutil.copy2(req.path, path)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 475, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 273, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 164, in _fastcopy_sendfile
    raise err from None
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 150, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device: '/home/kilian/In-Context-SSM/wandb/run-20240523_184418-bj23rskc/files/media/plotly/function_0_5791_997999dceb50026f948a.plotly.json' -> '/tmp/tmp3qf7aw27wandb/5wbms30z-media/plotly/function_0_5791_997999dceb50026f948a.plotly.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_checksum.py", line 87, in _thread_body
    shutil.copy2(req.path, path)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 475, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 283, in copyfile
    copyfileobj(fsrc, fdst)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 204, in copyfileobj
    fdst_write(buf)
OSError: [Errno 28] No space left on device
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/data_types/plotly.py", line 72, in __init__
    util.json_dump_safer(val, fp)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/util.py", line 823, in json_dump_safer
    return dump(obj, fp, cls=WandBJSONEncoder, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/json/__init__.py", line 180, in dump
    fp.write(chunk)
  File "<frozen codecs>", line 727, in write
  File "<frozen codecs>", line 378, in write
OSError: [Errno 28] No space left on device

During handling of the above excTraceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 1, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240523_233150-jukqqh1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/jukqqh1n
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 132, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 111, in train
    for i, (_,y) in enumerate(dataloader_train):
           ^^^^^
ValueError: too many values to unpack (expected 2)
wandb: - 0.004 MB of 0.004 MB uploaded
wandb: \ 0.019 MB of 0.019 MB uploaded
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/jukqqh1n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240523_233150-jukqqh1n/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240523_233600-uj5lz8zx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/uj5lz8zx
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 142, in <module>
    set_seeds(42)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 14, in set_seeds
    random.seed(seed)
    ^^^^^^
NameError: name 'random' is not defined. Did you forget to import 'random'?
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_092023-7cg19gbe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/7cg19gbe
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 146, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 118, in train
    load_checkpoint(config, model, opt)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 68, in load_checkpoint
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for HiPPO_LegT:
	size mismatch for C_discr: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for A: copying a param with shape torch.Size([10, 10]) from checkpoint, the shape in current model is torch.Size([32, 32]).
	size mismatch for B: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([32]).
wandb: - 0.004 MB of 0.004 MB uploaded
wandb: \ 0.019 MB of 0.019 MB uploaded
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/7cg19gbe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240524_092023-7cg19gbe/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_102333-40hj5j7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/40hj5j7k
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

Exception in thread Thread-10 (_thread_body):
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_checksum.py", line 80, in _thread_body
    filesystem.mkdir_exists_ok(os.path.dirname(path))
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/lib/filesystem.py", line 30, in mkdir_exists_ok
    os.makedirs(dir_name, exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 225, in makedirs
OSError: [Errno 28] No space left on device: '/tmp/tmpzd1hbgi5wandb/a9i0dvkr-media'
--- Logging error ---
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1164, in emit
    self.flush()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1144, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 48, in run
    self._run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 99, in _run
    self._process(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal.py", line 327, in _process
    self._sm.send(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/sender.py", line 385, in send
    send_handler(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/sender.py", line 405, in send_request
    logger.debug(f"send_request: {request_type}")
Message: 'send_request: summary_record'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1164, in emit
    self.flush()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1144, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
--- Logging error ---
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
wandb: / 24.312 MB of 24.312 MB uploaded
wandb: - 24.312 MB of 24.312 MB uploaded
wandb: \ 24.312 MB of 24.312 MB uploaded
wandb: | 24.312 MB of 24.312 MB uploaded
--- Logging error ---
wandb: / 24.312 MB of 24.312 MB uploaded
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1164, in emit
    self.flush()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1144, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 48, in run
    self._run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 99, in _run
    self._process(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal.py", line 327, in _process
    self._sm.send(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/sender.py", line 385, in send
    send_handler(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/sender.py", line 405, in send_request
    logger.debug(f"send_request: {request_type}")
Message: 'send_request: poll_exit'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1164, in emit
    self.flush()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/__init__.py", line 1144, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 48, in run
    self._run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal_util.py", line 99, in _run
    self._process(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/internal.py", line 278, in _process
    self._hm.handle(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/handler.py", line 150, in handle
    handler(record)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/sdk/internal/handler.py", line 158, in handle_request
    logger.debug(f"handle_request: {request_type}")
Message: 'handle_request: status_report'
Arguments: ()
wandb: - 24.312 MB of 24.312 MB uploaded
--- Logging error ---
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/logging/_wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_114055-hl7q1hbi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/hl7q1hbi
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_114222-simkys38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/simkys38
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_114444-doa4yp8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/doa4yp8p
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_120409-95x489eq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/95x489eq
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_121119-k1lww0kb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/k1lww0kb
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 146, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 134, in train
    test(config, dataloader_test, model)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 29, in wrapper
    return func(config, dataloader, model, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 78, in test
    for i, y in enumerate(dataloader):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    index = self._next_index()  # may raise StopIteration
            ^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 621, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/sampler.py", line 287, in __iter__
    for idx in self.sampler:
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/sampler.py", line 111, in __iter__
    return iter(range(len(self.data_source)))
                      ^^^^^^^^^^^^^^^^^^^^^
TypeError: object of type 'BrownianMotionDataset' has no len()
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.020 MB uploadedwandb: 
wandb: Run history:
wandb: loss ‚ñÅ
wandb: 
wandb: Run summary:
wandb: loss 0.0
wandb: 
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/k1lww0kb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240524_121119-k1lww0kb/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_121247-gbgp1rmz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/gbgp1rmz
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

Exception in thread Thread-11 (_thread_body):
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_upload.py", line 118, in _thread_body
    self._handle_event(event)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_upload.py", line 180, in _handle_event
    self._start_upload_job(event)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_upload.py", line 192, in _start_upload_job
    self._spawn_upload(event)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_upload.py", line 227, in _spawn_upload
    self._pool.submit(run_and_notify)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/concurrent/futures/thread.py", line 170, in submit
    raise RuntimeError('cannot schedule new futures after shutdown')
RuntimeError: cannot schedule new futures after shutdown
Failed to upload file: /tmp/tmppsv4qkn0wandb/5p21qv2y-media/plotly/function_0_183_0b389a0e3f42b6c505a4.plotly.json
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/upload_job.py", line 123, in push
    with open(self.save_path, "rb") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmppsv4qkn0wandb/5p21qv2y-media/plotly/function_0_183_0b389a0e3f42b6c505a4.plotly.json'
Failed to upload file: /tmp/tmppsv4qkn0wandb/yryp3yy9-media/plotly/function_0_184_9eb63ee934e986e290e8.plotly.json
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/upload_job.py", line 123, in push
    with open(self.save_path, "rb") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmppsv4qkn0wandb/yryp3yy9-media/plotly/function_0_184_9eb63ee934e986e290e8.plotly.json'
wandb: ERROR Error uploading "media/plotly/function_0_183_0b389a0e3f42b6c505a4.plotly.json": FileNotFoundError, [Errno 2] No such file or directory: '/tmp/tmppsv4qkn0wandb/5p21qv2y-media/plotly/function_0_183_0b389a0e3f42b6c505a4.plotly.json'
wandb: ERROR Error uploading "media/plotly/function_0_184_9eb63ee934e986e290e8.plotly.json": FileNotFoundError, [Errno 2] No such file or directory: '/tmp/tmppsv4qkn0wandb/yryp3yy9-media/plotly/function_0_184_9eb63ee934e986e290e8.plotly.json'
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_122223-p289d7r7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/p289d7r7
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_123543-0td0qc2b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/0td0qc2b
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_123736-pvx47btq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/pvx47btq
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_125941-8s3oaut6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/8s3oaut6
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_130622-r2rd778k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/r2rd778k
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_131438-e7jzrgxv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/e7jzrgxv
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

Exception in thread Thread-10 (_thread_body):
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_checksum.py", line 84, in _thread_body
    shutil.copy2(req.path, path)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 475, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 273, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 164, in _fastcopy_sendfile
    raise err from None
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 150, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device: '/home/kilian/In-Context-SSM/wandb/run-20240524_131438-e7jzrgxv/files/media/plotly/function_0_5870_2cf095871cdcd143eab9.plotly.json' -> '/tmp/tmpwwhqzuijwandb/4saqcs04-media/plotly/function_0_5870_2cf095871cdcd143eab9.plotly.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/wandb/filesync/step_checksum.py", line 87, in _thread_body
    shutil.copy2(req.path, path)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 475, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 283, in copyfile
    copyfileobj(fsrc, fdst)
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/shutil.py", line 204, in copyfileobj
    fdst_write(buf)
OSError: [Errno 28] No space left on device
Traceback (most recent call last):
  File "/home/kilian/miniconda3/envs/vmamba/libwandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240524_184850-j88jtoak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/j88jtoak
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_001337-cmdn3u9a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/cmdn3u9a
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_001856-t8xgxokv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/t8xgxokv
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

wandb: - 129.858 MB of 129.858 MB uploadedwandb: \ 129.858 MB of 129.883 MB uploadedwandb: | 129.883 MB of 129.883 MB uploadedwandb: / 129.883 MB of 129.883 MB uploadedwandb: 
wandb: Run history:
wandb:   dist_1_C ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà
wandb:   dist_1_D ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   dist_2_C ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:   dist_2_D ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: dist_inf_C ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà
wandb: dist_inf_D ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       loss ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  test_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   dist_1_C 553.10944
wandb:   dist_1_D 3.53447
wandb:   dist_2_C 75.06062
wandb:   dist_2_D 3.53447
wandb: dist_inf_C 23.41638
wandb: dist_inf_D 3.53447
wandb:       loss 0.00028
wandb:  test_loss 0.19262
wandb: 
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/t8xgxokv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 1500 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_001856-t8xgxokv/logs
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 152, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 104, in train
    dataset = get_datasets(config=config, test=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 30, in get_datasets
    raise ValueError("Unknown dataset")
ValueError: Unknown dataset
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_122733-9cgogr3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/9cgogr3d
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_133352-exsx732g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/exsx732g
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 152, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 129, in train
    y_hat = model(y) # signal y is 0,1,2,3,4,5..., N / y_hat is 1,2,3,4,5,6..., N+1
            ^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/model/hippo.py", line 92, in forward
    u = inputs * self.B # (length, ..., N)
                 ^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'HiPPO_LegT' object has no attribute 'B'
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.020 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/exsx732g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_133352-exsx732g/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_133925-nhtv6tae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/nhtv6tae
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_143806-j2yewv10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/j2yewv10
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_144434-p9j03fg4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/p9j03fg4
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 125, in train
    model = HiPPO_LegT(N=config["model"]["rank"], dt=1/config["train"]["data"]["num_points"], teacher_ratio=config["train"]["teacher_ratio"], trainable=True, full=config["model"]["full"])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/model/hippo.py", line 76, in __init__
    A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/scipy/signal/_lti_conversion.py", line 470, in cont2discrete
    return cont2discrete(system, dt, method="gbt", alpha=0.5)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/scipy/signal/_lti_conversion.py", line 465, in cont2discrete
    cd = linalg.solve(ima.transpose(), c.transpose())
                                       ^^^^^^^^^^^^^
TypeError: transpose() received an invalid combination of arguments - got (), but expected one of:
 * (int dim0, int dim1)
 * (name dim0, name dim1)

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 125, in train
    model = HiPPO_LegT(N=config["model"]["rank"], dt=1/config["train"]["data"]["num_points"], teacher_ratio=config["train"]["teacher_ratio"], trainable=True, full=config["model"]["full"])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/model/hippo.py", line 76, in __init__
    A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/scipy/signal/_lti_conversion.py", line 470, in cont2discrete
    return cont2discrete(system, dt, method="gbt", alpha=0.5)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/scipy/signal/_lti_conversion.py", line 465, in cont2discrete
    cd = linalg.solve(ima.transpose(), c.transpose())
                                       ^^^^^^^^^^^^^
TypeError: transpose() received an invalid combination of arguments - got (), but expected one of:
 * (int dim0, int dim1)
 * (name dim0, name dim1)

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 171, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 125, in train
    model = HiPPO_LegT(N=config["model"]["rank"], dt=1/config["train"]["data"]["num_points"], teacher_ratio=config["train"]["teacher_ratio"], trainable=True, full=config["model"]["full"])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: HiPPO_LegT.__init__() got an unexpected keyword argument 'full'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 171, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 125, in train
    model = HiPPO_LegT(N=config["model"]["rank"], dt=1/config["train"]["data"]["num_points"], teacher_ratio=config["train"]["teacher_ratio"], trainable=True, full=config["model"]["full"])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: HiPPO_LegT.__init__() got an unexpected keyword argument 'full'
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240525_221349-45zf0org
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/45zf0org
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_111424-98z0xgwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/98z0xgwl
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 140, in train
    load_checkpoint(config, model, opt)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 85, in load_checkpoint
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for HiPPO_LegT:
	size mismatch for C_discr: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for A: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for B: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.007 MB of 0.019 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/98z0xgwl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_111424-98z0xgwl/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_113130-m2jedy50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/m2jedy50
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 140, in train
    load_checkpoint(config, model, opt)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 85, in load_checkpoint
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for HiPPO_LegT:
	size mismatch for C_discr: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for A: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for B: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.019 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/m2jedy50
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_113130-m2jedy50/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_113204-zs298mua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/zs298mua
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 148, in train
    loss.backward()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 1000; expected version 999 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.009 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/zs298mua
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_113204-zs298mua/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_113354-8iv24mma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/8iv24mma
/home/kilian/In-Context-SSM/icl_learning/data.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 148, in train
    loss.backward()
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 1000; expected version 999 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/8iv24mma
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_113354-8iv24mma/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_120106-062atk9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/062atk9n
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 144, in train
    for i, y in enumerate(dataloader_train):
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 160, in __getitem__
    offset = np.random.uniform(-float('inf')/2, float('inf')/2)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "numpy/random/mtrand.pyx", line 1156, in numpy.random.mtrand.RandomState.uniform
OverflowError: Range exceeds valid bounds
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.020 MB uploadedwandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/062atk9n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_120106-062atk9n/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_120717-1hrkoo9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/1hrkoo9q
/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_123012-cg785ccv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/cg785ccv
/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_123049-ftsjjamw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/ftsjjamw
/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_123459-ynahf7ap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/ynahf7ap
/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_123744-wizvblzd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/wizvblzd
/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:163: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 169, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 140, in train
    load_checkpoint(config, model, opt)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 85, in load_checkpoint
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for HiPPO_LegT:
	size mismatch for C_discr: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for A: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).
	size mismatch for B: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
wandb: - 257.632 MB of 257.632 MB uploadedwandb: \ 257.651 MB of 257.651 MB uploadedwandb: | 257.651 MB of 257.651 MB uploadedwandb: 
wandb: Run history:
wandb:   dist_1_C ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:   dist_1_D ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   dist_2_C ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:   dist_2_D ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: dist_inf_C ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: dist_inf_D ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  test_loss ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   dist_1_C 465.08478
wandb:   dist_1_D 3.30326
wandb:   dist_2_C 63.1896
wandb:   dist_2_D 3.30326
wandb: dist_inf_C 10.56644
wandb: dist_inf_D 3.30326
wandb:       loss 296.54472
wandb:  test_loss 325.15447
wandb: 
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/wizvblzd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 3000 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_123744-wizvblzd/logs
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240526_132857-lvm6uwjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/lvm6uwjk
/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(sine).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:142: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: - 1311.569 MB of 1311.569 MB uploadedwandb: \ 1311.569 MB of 1311.569 MB uploadedwandb: | 1311.569 MB of 1311.569 MB uploadedwandb: 
wandb: Run history:
wandb:   dist_1_C ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñà‚ñà
wandb:   dist_1_D ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà
wandb:   dist_2_C ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñà‚ñà
wandb:   dist_2_D ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà
wandb: dist_inf_C ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñà‚ñà
wandb: dist_inf_D ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà
wandb:       loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:  test_loss ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:   dist_1_C 2123.69019
wandb:   dist_1_D 2.06166
wandb:   dist_2_C 143.63095
wandb:   dist_2_D 2.06166
wandb: dist_inf_C 12.07453
wandb: dist_inf_D 2.06166
wandb:       loss 0.00509
wandb:  test_loss 0.59583
wandb: 
wandb: üöÄ View run HiPPO_LegT at: https://wandb.ai/incontextssm/incontextssm/runs/lvm6uwjk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/incontextssm/incontextssm
wandb: Synced 6 W&B file(s), 15000 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_132857-lvm6uwjk/logs
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 193, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 142, in train
    dataset = get_datasets(config=config, test=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 44, in get_datasets
    return MixedDataset(num_points=config["train"]["data"]["num_points"], num_functions=config["train"]["data"]["num_functions"], config=config, device=config["device"])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 244, in __init__
    self.sine_ds = MultipleSineDataset(**dataconfig, device=device, test=test)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: MultipleSineDataset.__init__() missing 2 required positional arguments: 'num_summands' and 'max_frequency'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 193, in <module>
    train(config)
  File "/home/kilian/In-Context-SSM/icl_learning/train.py", line 142, in train
    dataset = get_datasets(config=config, test=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 44, in get_datasets
    return MixedDataset(num_points=config["train"]["data"]["num_points"], num_functions=config["train"]["data"]["num_functions"], config=config, device=config["device"])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kilian/In-Context-SSM/icl_learning/data.py", line 244, in __init__
    self.sine_ds = MultipleSineDataset(**dataconfig, device=device, test=test)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: MultipleSineDataset.__init__() missing 2 required positional arguments: 'num_summands' and 'max_frequency'
wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240528_091633-zjoiuy11
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/zjoiuy11
/home/kilian/In-Context-SSM/icl_learning/data.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:192: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:150: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:171: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

wandb: Currently logged in as: khaefeli (incontextssm). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/kilian/In-Context-SSM/wandb/run-20240528_103336-bnwd6ser
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run HiPPO_LegT
wandb: ‚≠êÔ∏è View project at https://wandb.ai/incontextssm/incontextssm
wandb: üöÄ View run at https://wandb.ai/incontextssm/incontextssm/runs/bnwd6ser
/home/kilian/In-Context-SSM/icl_learning/data.py:192: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y).to(torch.float32).to(self.device)
/home/kilian/miniconda3/envs/vmamba/lib/python3.12/site-packages/plotly/matplotlylib/renderer.py:609: UserWarning:

I found a path object that I don't think is part of a bar chart. Ignoring.

/home/kilian/In-Context-SSM/icl_learning/data.py:192: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:150: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

/home/kilian/In-Context-SSM/icl_learning/data.py:171: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).

